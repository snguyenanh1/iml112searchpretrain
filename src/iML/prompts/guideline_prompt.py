# src/iML/prompts/guideline_prompt.py
import json
from typing import Dict, Any

from .base_prompt import BasePrompt

def _create_variables_summary(variables: dict) -> dict:
    """Create a concise summary for variables in the profile."""
    summary = {}
    for var_name, var_details in variables.items():
        summary[var_name] = {
            "type": var_details.get("type"),
            "n_unique": var_details.get("n_unique"),
            "p_missing": var_details.get("p_missing"),
            "mean": var_details.get("mean"),
            "std": var_details.get("std"),
            "min": var_details.get("min"),
            "max": var_details.get("max"),
        }
    return summary

class GuidelinePrompt(BasePrompt):
    """
    Prompt handler to create guidelines for AutoML pipeline.
    """

    def default_template(self) -> str:
        """Default template to request LLM to create guidelines."""
        return """You are an expert Machine Learning architect. Your task is to analyze the provided dataset information and create a specific, actionable, and justified guideline for an AutoML pipeline.
## Dataset Information:
- Dataset: {dataset_name}
- Task: {task_desc}
- Size: {n_rows:,} rows, {n_cols} columns
- Key Quality Alerts: {alerts}
- Output format: {output_data}
- Submission file description: {submission_file_description}

## Variables Analysis Summary:
```json
{variables_summary_str}
```

{id_format_section}

## IMPORTANT CONSTRAINTS FOR REPRODUCIBILITY:
- ALWAYS use random_state=42 for ALL random operations (train_test_split, cross_validation, model initialization)
- Use simple random split strategy for train/test splitting

## Guideline Generation Principles & Examples
Your response must be guided by the following principles. Refer to these examples to understand the required level of detail.

BE SPECIFIC AND ACTIONABLE: Your recommendations must be concrete actions.
- Bad (Generic): "Handle missing values"
- Good (Specific): "Impute 'Age' with the median"

JUSTIFY YOUR CHOICES INTERNALLY: Even if the final JSON does not include every reasoning detail, your internal decision process must be sound, based on the data properties.

IT IS ACCEPTABLE TO OMIT: If a step is not necessary, provide an empty list or null for that key in the JSON output.

High-Quality Examples:

Example 1: Feature Engineering for a DateTime column
For a DateTime column like 'transaction_date', a good feature_engineering list would be ["Extract 'month' from 'transaction_date'", "Extract 'day_of_week' from 'transaction_date'"].

Example 2: Handling High Cardinality Categorical Data
For a categorical column 'product_id' with over 100 unique values, a good recommendation is ["Apply frequency encoding to 'product_id'"].

Example 3: Handling Missing Numerical Data
For a numeric column 'income' with 25% missing values and a skewed distribution, a good recommendation is ["Impute 'income' with its median"].

{algorithm_constraint}

Before generating the final JSON, consider:
1. Identify the target variable and task type (classification, regression, etc.).
2. Review each variable's type, statistics, and potential issues.
3. Choose appropriate and reasonable preprocessing steps for that pretrained model.
4. If using pretrained models, choose the most appropriate ones.
5. Compile these specific actions into the required JSON format.


Output Format: Your response must be in the JSON format below:
Provide your response in JSON format. An empty list or null is acceptable for recommendations if not applicable.

IMPORTANT: Ensure the generated JSON is perfectly valid.
- All strings must be enclosed in double quotes.
- All backslashes inside strings must be properly escaped.
- There should be no unescaped newline characters within a string value.
- Do not include comments within the JSON output.

{{
    "target_identification": {{
        "target_variable": "identified_target_column_name",
        "reasoning": "explanation for target selection",
        "task_type": "classification/regression/etc"
    }},
    "modeling": {{
        "recommended_algorithms": ["one most suitable algorithm"],
        "model_selection": ["model_name1"](pretrained model name if using pretrained model),
        "eval_metrics": [],
        "random_state": 42,
        "notes": "additional notes",
        "IDs in submission file contain file extensions": "true/false"
    }},
    "preprocessing": {{
        "data_cleaning": ["specific step 1", "specific step 2"],
        "feature_engineering": ["specific technique 1", "specific technique 2"],
        "missing_values": ["strategy 1", "strategy 2"],
        "feature_selection": ["method 1", "method 2"],
        "data_splitting": {{"train": 0.8, "val": 0.2, "strategy": "simple_random", "random_state": 42}},
        "notes": "additional notes"
        "IDs in train file contain file extensions": "true/false"
    }},
    "evaluation": {{
        "metrics": ["metric 1", "metric 2"],
        "validation_strategy": ["3-fold cross-validation"],
        "performance_benchmarking": ["baseline 1", "baseline 2"],
        "result_interpretation": ["interpretation 1", "interpretation 2"]
    }}
}}"""

    def build(self, description_analysis: Dict[str, Any], profiling_result: Dict[str, Any], model_suggestions: Dict[str, Any] | None = None, iteration_type: str | None = None) -> str:
        """Build prompt from analysis and profiling results.

        Supports two formats:
        - Summarized profiling (preferred): keys include 'files', 'label_analysis', 'feature_quality'.
        - Raw profiling (fallback): keys include 'summaries', 'profiles'.
        """
        task_info = description_analysis

        dataset_name = task_info.get('name', 'N/A')
        task_desc = task_info.get('task', 'N/A')
        output_data = task_info.get('output_data', 'N/A')
        submission_file_description = task_info.get('submission file description', 'N/A')
        n_rows = 0
        n_cols = 0
        alerts_out = []
        variables_summary_dict = {}

        if 'label_analysis' in profiling_result or 'files' in profiling_result:
            # Summarized format
            files = profiling_result.get('files', []) or []
            # choose train-like file if present
            chosen = None
            for f in files:
                name = (f.get('name') or '').lower()
                if 'train' in name and 'test' not in name and 'submission' not in name:
                    chosen = f
                    break
            if not chosen and files:
                chosen = files[0]
            if chosen:
                n_rows = chosen.get('n_rows', 0) or 0
                n_cols = chosen.get('n_cols', 0) or 0

            la = profiling_result.get('label_analysis', {}) or {}
            fq = profiling_result.get('feature_quality', {}) or {}

            # alerts: concise messages
            if la:
                if la.get('has_label_column') is False:
                    alerts_out.append('No label column detected')
                if la.get('has_missing_labels'):
                    alerts_out.append('Missing labels present')
                imb = la.get('class_distribution_imbalance')
                if imb and imb != 'none':
                    alerts_out.append(f'label imbalance: {imb}')
                if la.get('num_classes'):
                    alerts_out.append(f"num_classes={la['num_classes']}")

            if fq:
                hm = fq.get('high_missing_columns') or []
                if hm:
                    alerts_out.append(f"high-missing cols: {len(hm)}")
                hc = fq.get('high_cardinality_categoricals') or []
                if hc:
                    alerts_out.append(f"high-cardinality cats: {len(hc)}")

            # variables summary minimal to avoid noise
            variables_summary_dict = {
                'high_missing_columns': fq.get('high_missing_columns') or [],
                'high_cardinality_categoricals': fq.get('high_cardinality_categoricals') or [],
                'date_like_cols': fq.get('date_like_cols') or [],
                'label_column': la.get('label_column'),
            }
        else:
            # Fallback to raw profiling (legacy)
            train_key = None
            for key in profiling_result.get('summaries', {}).keys():
                if 'test' not in key.lower() and 'submission' not in key.lower():
                    train_key = key
                    break
            if not train_key:
                train_key = next(iter(profiling_result.get('summaries', {})), None)

            train_summary = profiling_result.get('summaries', {}).get(train_key, {})
            train_profile = profiling_result.get('profiles', {}).get(train_key, {})
            n_rows = train_summary.get('n_rows', 0)
            n_cols = train_summary.get('n_cols', 0)
            alerts = train_profile.get('alerts', [])
            variables = train_profile.get('variables', {})
            alerts_out = alerts[:3] if alerts else []
            variables_summary_dict = _create_variables_summary(variables)

        # Build auxiliary sections
        variables_summary_str = json.dumps(variables_summary_dict, indent=2, ensure_ascii=False)
        model_suggestions = model_suggestions or {}
        # Extract SOTA models (from ADK) if present
        sota_models = model_suggestions.get('sota_models', []) or []
        model_suggestions_str = json.dumps(model_suggestions, indent=2, ensure_ascii=False)

        # Generate ID format section
        id_format_section = self._generate_id_format_section(profiling_result)

        # Generate algorithm constraint based on iteration type
        algorithm_constraint = self._get_algorithm_constraint(iteration_type)

        # If pretrained iteration and SOTA models exist, add a hard requirement block
        sota_section = ""
        if iteration_type == "pretrained" and sota_models:
            # Keep only lightweight view for the prompt
            shortlist = [
                {
                    "model_name": m.get("model_name"),
                    "model_link": m.get("model_link"),
                }
                for m in sota_models[:10]
            ]
            sota_section = (
                "\n## SOTA MODEL SHORTLIST (from ADK search)\n"
                + json.dumps(shortlist, indent=2, ensure_ascii=False)
                + "\n\nIMPORTANT (PRETRAINED): You MUST choose a model from the SOTA shortlist above (or its exact HF model) for 'model_selection'.\n"
                  "Provide configuration aligned with the chosen model."
            )

        prompt = self.template.format(
            dataset_name=dataset_name,
            task_desc=task_desc,
            n_rows=n_rows,
            n_cols=n_cols,
            alerts=alerts_out if alerts_out else 'None',
            variables_summary_str=variables_summary_str,
            output_data=output_data,
            submission_file_description=submission_file_description,
            model_suggestions_str=model_suggestions_str,
            algorithm_constraint=algorithm_constraint,
            id_format_section=id_format_section + sota_section
        )

        self.manager.save_and_log_states(prompt, "guideline/guideline_prompt.txt")
        return prompt
    
    def _get_algorithm_constraint(self, iteration_type: str | None) -> str:
        """Get algorithm constraint based on iteration type."""
        if iteration_type == "traditional":
            return "IMPORTANT: YOU MUST USE TRADITIONAL ML ALGORITHMS: XGBoost, LightGBM, CatBoost, Linear regression, SVM, Bayes, ..."
        elif iteration_type == "custom_nn":
            return "IMPORTANT: YOU MUST BUILD CUSTOM NEURAL NETWORKS from scratch using PyTorch. "
        elif iteration_type == "pretrained":
            return "IMPORTANT: YOU MUST USE PRETRAINED MODELS"
        else:
            # Default for backward compatibility
            return "None"

    def _generate_id_format_section(self, profiling_result: Dict[str, Any]) -> str:
        """Generate ID format analysis section for the prompt."""
        # Check if we have ID format analysis
        id_format_analysis = profiling_result.get('id_format_analysis', {})
        
        if not id_format_analysis:
            return ""
        
        has_extensions = id_format_analysis.get('has_file_extensions', False)
        detected_extensions = id_format_analysis.get('detected_extensions', [])
        format_notes = id_format_analysis.get('format_notes', [])
        submission_analysis = id_format_analysis.get('submission_format_analysis')
        
        if not has_extensions and not format_notes:
            return ""
        
        section_lines = ["## ID FORMAT ANALYSIS:"]
        
        if has_extensions:
            section_lines.append(f"- **ID columns contain file extensions**: {', '.join(detected_extensions)}")
        
        if submission_analysis:
            submission_has_ext = submission_analysis.get('submission_has_extensions', False)
            submission_file = submission_analysis.get('submission_file', 'N/A')
            section_lines.append(f"- **Submission format detected**: File extensions {'required' if submission_has_ext else 'NOT required'} in {submission_file}")
        
        if format_notes:
            section_lines.append("- **CRITICAL NOTES**:")
            for note in format_notes:
                section_lines.append(f"  * {note}")
        
        section_lines.extend([
            "",
            "**PREPROCESSING NOTE**: If ID format mismatch detected, ensure preprocessing handles ID transformation correctly.",
            "**MODELING NOTE**: When creating submission files, ensure ID format matches exactly what's expected.",
            ""
        ])
        
        return "\n".join(section_lines)

    def parse(self, response: str) -> Dict[str, Any]:
        """Parse JSON response from LLM."""
        try:
            parsed_response = json.loads(response.strip().replace("```json", "").replace("```", ""))
        except json.JSONDecodeError as e:
            self.manager.logger.error(f"Failed to parse JSON from LLM response for guideline: {e}")
            parsed_response = {"error": "Invalid JSON response from LLM", "raw_response": response}
        
        self.manager.save_and_log_states(
            json.dumps(parsed_response, indent=4, ensure_ascii=False), 
            "guideline/guideline_response.json"
        )
        return parsed_response
